>不太熟悉[音频信号处理](https://www.zhihu.com/search?q=%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%223397367727%22%7D)的内容，所以没法解答这个相关的问题，但是对于赛题其他的还是有一定见解
首先呢，一般要将算法移植到FPGA是需要定点化处理的，用于降低延迟，提高速度的。所以需要学习浮点，[定点转换](https://www.zhihu.com/search?q=%E5%AE%9A%E7%82%B9%E8%BD%AC%E6%8D%A2&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%223397367727%22%7D)，还有各种量化方法。
其次，[信号处理算法](https://www.zhihu.com/search?q=%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%223397367727%22%7D)（或者算法的部分过程）是可以流水线优化的，那么就涉及到算法的逻辑拆分，流水处理，这是算法移植到FPGA的重中之重！
然后，因为FPGA的并行性，一般可以把算法的部分大规模运算并行处理，例如dsp的[滤波器](https://www.zhihu.com/search?q=%E6%BB%A4%E6%B3%A2%E5%99%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%223397367727%22%7D)滑窗乘累加
最后，算法移植的万能流程:
先要把算法Python或MATLAB实现无误。
然后再决定哪些流程串行，哪些流程可以并行加速，有没有不是定向运行的步骤（比如根据条件跳转执行方式）需要用[状态机](https://www.zhihu.com/search?q=%E7%8A%B6%E6%80%81%E6%9C%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%223397367727%22%7D)处理跳转等等，确定好信号输入输出的接口（握手，interrupt，ack等），按步骤划分模块，确定模块间耦合的方式（有没有握手，有没有反压），写成文档
然后就一个个把模块写verilog实现出来，仿真比对处理结果（用MATLAB[辅助检验](https://www.zhihu.com/search?q=%E8%BE%85%E5%8A%A9%E6%A3%80%E9%AA%8C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%223397367727%22%7D)），然后连接模块，封装接口

#  功能
1. 音频降噪
2. 实时人声调整
3. 音频回声消除
4. 音乐与人声分离
5. 实时人物音频画像
6. 音频分类
7. 声纹识别
8. 变声检测

# 技术
1. FPGA实现滤波器的功能，实现滤波器从MATLAB移植到FPGA上，需要对数据定点化的能力。
2. 神经网络模型部署到FPGA上，目前有具体部署模型到FPGA上的主要是使用了xlinx 和 intel家的工具，不确定能否直接得到verilog代码
3. PCIE与上位机通信
4. 上位机软件编写

  
[GitHub - facebookresearch/denoiser: Real Time Speech Enhancement in the Waveform Domain (Interspeech 2020)We provide a PyTorch implementation of the paper Real Time Speech Enhancement in the Waveform Domain. In which, we present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections. It is optimized on both time and frequency domains, using multiple loss functions. Empirical evidence shows that it is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb. Additionally, we suggest a set of data augmentation techniques applied directly on the raw waveform which further improve model performance and its generalization abilities.](https://github.com/facebookresearch/denoiser)
facebook的音频降噪库，效果还是可以的，火车汽笛的声音已经呗降低到比较小的程度了


```
python -m denoiser.enhance --model_path=F:\\FPGA\\denoiser-main\\outputs\\exp_\\best.th --noisy_dir=F:\\FPGA\\denoiser-main\\my_data\\noisy --out_dir=F:\\FPGA\\denoiser-main\\my_data\\clean
```